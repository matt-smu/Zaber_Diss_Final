% Observation is the foundation of scientific experimentation, and it becomes a measurement when they are quantified with respect to an agreed upon scale, or measurement unit. 
A number of metrics have been proposed in the literature which attempt to quantify some property of cyber security, but no systematic validation has been conducted to characterize the behaviour of these metrics as measurement instruments, or to understand how the quantity being measured is related to the security of the system under test. In Chapter \ref{ch:background} we broadly classified the body of available security metrics against the recently released Cyber Security Body of Knowledge, and identified common attributes across metric classes which may be useful anchors for comparison. In this chapter we propose a general four stage evaluation pipeline to encapsulate the processing specifics of each metric, encouraging a separation of the actual measurement logic from the model it is often paired with in publication. Decoupling these stages allows us to systematically apply a range of input models to a set of metrics, and we demonstrate some important results in our proof of concept. First, we determine a metric's suitability for use as a measurement instrument against validation criteria like operational range, sensitivity, and precision by observing performance over controlled variations of a reference input. Then we show how evaluating multiple metrics against common reference sets allows direct comparison of results and identification of patterns in measurement performance. Consequently, development and operations teams can also use this strategy to evaluate security tradeoffs between competing input designs (which we show in the case study in Section \ref{sec:case_studies:att}) or to measure the effects of incremental changes during production deployments (which we show in Chapter \ref{ch:benchmarking}). 

% The motivation of this thesis is to make modern information systems more secure, and the driving force behind that goal is automation. Many of the problems addressed in this work stem from the disparate ecosystem of tools, APIs, methodologies, libraries, and frameworks that exist in relative isolation to one another. Consider Security Information and Event Management (SIEM) systems as an example, which provide correlation of host/network event logs, IDS/IPS alerts, threat/vulnerability feeds, etc, and present a unified view of the systemâ€™s security posture automatically to the SOC. Before the advent of managed SIEMs, sys admins typically filled the role of security engineers, and relied on hand rolled collections of shell/perl scripts to manage systems, parse logs, collect or push events, format reports, and issue alarms. To be effective required tribal knowledge along with proficiency in programming, network plumbing, and systems management, so changes to the environment or workforce made it extremely difficult(expensive) to deliver continuous monitoring capabilities to operators at any scale. 

% We are in a similar state today with network design and enterprise planning. Infrastructure-as-Code, SDN, virtualization and containerization are all critical components in modern deployments, but the glue that ties them together is largely ad-hoc, and risk evaluation is still a manual task. In order to understand the security posture of a system even before it is rolled out and SIEMs are in place, we have created a tool to facilitate the automated analysis, collection, correlation, and dissemination of the security metrics mentioned above. The necessity of such a tool is critical to evaluating the efficacy of the metrics reviewed above, and provides the foundation for ongoing research in machine learning models for secure systems planning, design, and evolution as demonstrated in Chapter \ref{ch:ml}.


% If we measure an aspect of security before and after a change takes place, then we can quantify the impact that change had on security. If we test an aspect of cyber security at regular intervals, then we can determine the average rate of change for that security metric over the given time period. In order to sample security measurements at regular (approaching continuous) intervals, we assert that the test apparatus must be fully automated. Using the 5 basic CyBOK categories as a guide, we can briefly identify some types and familiar sources of available security metrics and describe the suitability to automation for each.

% \begin{theorem}\label{theo:intro:secmet_diff}
% If we measure an aspect of security before and after a change takes place, then we can quantify the impact that change had on security.
% \end{theorem}

% \begin{theorem}\label{theo:intro:secmet_rate}
% If we test an aspect of cyber security at regular intervals, then we can determine the average rate of change for that security metric over the given time period.
% \end{theorem}

% Theorems \ref{theo:intro:secmet_diff} and \ref{theo:intro:secmet_rate} establish the basis for a \textit{cyber security calculus} that are a central premise of this work. 
