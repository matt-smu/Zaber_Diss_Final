
Organizations that manage their own infrastructure today, whether physical or virtual, have a bevy of tools available to characterize the ongoing health and security of their network. Telemetry data provides heuristics for anomaly detection, vulnerability feeds identify signatures of potential attack targets, and security information and event management(SIEM) systems aggregate these sources to provide continuous monitoring and alerting for incident response. 

Maintaining awareness of the security posture for the current system configuration is understandably the highest priority for network operators and security officers. But network architectures are not fixed in time. Increasingly we observe traditional perimeters extending to include cloud hosted resources and custom appliances being replaced with software running on commodity hardware. Changes such as these may appear idempotent while planning for an upgrade or migration, but may in fact introduce security gaps when implemented. 

Prior research establishes the attack graph \cite{ Dacier_1994, Lippmann_Ingols_2005, Ou_Govindavajhala_Appel, Phillips_Swiler_1998, Sheyner_Haines_Jha_Lippmann_Wing_2002} as a representation of the connectivity between vulnerabilities in a system, where edges form a collection of potential paths an attacker can traverse to reach a target. \cite{Abraham_Nair_2015c, Idika_Bhargava_2012, Noel_Jajodia_2014} derive metrics with which to evaluate an attack graph including Bayesian and Markov methods for stochastic simulations, along with basic structural properties of the graph. While these works along with those cited in the literature review provide specific measures that can be taken from an attack graph, they are in large part specific to existing enterprise networks hosting vulnerable software. What is missing from the literature is a quantitative method of assessing the security of multiple \textit{incremental} architecture changes, which this work begins to address. As SDN and NFV capabilities mature and are adopted widely, networks will only become more fluid, and the necessity of an analysis of alternatives for each potential network changes is becoming more apparent. 

This paper presents research focused on determining the security implications of proposed architectural changes during infrastructure migration. Our primary contribution is a methodology and framework for a decision support system to compare the security of multiple systems. This work addresses a gap in the literature where analysis of competing alternatives is needed to evaluate the most secure transition strategy. 

As a case study we consider a carrier network upgrading from existing core switching and routing elements to a centrally controlled Software Defined Network (SDN). We adopt the open source attack graph generation tool MulVal\cite{Ou_Appel_2005} for our implementation as it is freely available open source software and widely cited in this research area. We follow the process described in the Cyber Security Analytics Framework\cite{Abraham_Nair_2015a} to build metrics for distinct network transition states, and use these metrics to evaluate the security posture of the proposed migration strategy. Which metrics are evaluated is configurable in the system, and we report several structural and probabilistic measurements in the results along with possible interpretations. 

We extend the scope of \cite{Abraham_2016} from the enterprise to include core networking elements and associated metrics. As Layer 2 and 3 attacks often are not exposed due to software vulnerabilities, but rather surface in the network as a result of protocol or configuration decisions, we augment the MulVal system model with interaction rules specifying conditions necessary to successfully exploit these attack classes. Similar extensions for infrastructure elements like switches and routers were also added to the model where their functionality was implicit before.

During the course of testing we developed a set of Ansible roles and plays to automate the process from end to end, allowing us to spin up a clean environment, run analyses, and publish results in a single command. This automation process was a key component to enabling rapid assessment of multiple competing input models and we hope it will reduce friction for other researchers in the community.

Our experiments show the utility of our framework for several applications, as a standalone tool used by a team during migration planning, as an integration into existing monitoring and reporting systems, or as a scoring function for an automated management tool. 

% \subsection{Contributions}
The remainder of the paper is structured as follows: Section \ref{subsec:background:main} provides an overview of existing research and specific background topics related to this work. Section \ref{subsec:contribs:main} describes this project's contributions to automating and extending previous research. Section \ref{subsec:approach:main} establishes the use case and traces the flow of the application by example. Section \ref{subsec:results:main} evaluates the resulting security measurements and discusses how they can be interpreted. Finally, we draw conclusions and describe future directions in section \ref{subsec:conclusion:main}.



