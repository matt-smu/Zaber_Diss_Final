

The number of security metrics available in the literature is somewhat overwhelming. Surveys of security metrics from many\cite{Bohme_Nowey_2008, Haque_Keffeler_Atkison_2017, Hecker_2008, Kordy_2013, Kundu_Ghosh_Chokshi_Ghosh_2012, Pendleton_Garcia-Lebron_Cho_Xu_2016, Ramos_Lazar_Filho_Rodrigues_2017, Rudolph_Schwarz_2012, Savola_2007, Tavallaee_Stakhanova_Ghorbani_2010, Verendel_2009,  Wagner_Eckhoff_2015} sources and perspectives have been conducted, resulting in a multitude of taxonomies for each declaring when and where and how a metric should be applied.  What is notably lacking from these surveys, and indeed from much of the literature reviewed, is any empirical evaluation of the values measured by a given metric. The study of security metrics lacks the context needed to support adoption. What we provide in this chapter is a mechanism to establish the needed context by answering how well do these metrics perform individually across a variety of scenarios, and how do they compare with each other for a given model. To build context for security metrics, we break up our experiments into two parts. 

The first part \textit{sizes} the metric, examining how it behaves across different types and scales of input networks. We apply the metrics implemented in Section \ref{sec:automation:secmet_impl} to a set of models representative of different deployment scenarios. These models include enterprise networks and core networks at scales we label small, medium, and large. The models  act as a reference set against which we can evaluate properties of the implemented metrics. Our primary questions are how do these metrics perform as the size of the system changes, and how do they perform in different types of systems. As our reference set grows, we can develop an understanding of the fundamental or universal security properties we can measure, along with the best metrics for a specific situation. 

The second part \textit{ranges} the metric, examining how it behaves as the security of the system under test changes. We select a system model from our standard set and generate an attack model for a scenario. The range of transition values in the attack model varies by metric, but we can, in the general case, fix these values to present a minimally and maximally secure model with respect to the chosen metric. This bounds the security of a system to a specific range, and measurements within this interval characterize the behaviour of the metric for a scenario. We make observations about monotonicity and sensitivity with respect to a metric and describe properties common across metrics. 

We consider ranging and sizing as two aspects of security metric benchmarking. By benchmarking security metrics we validate their fitness for use in general scenarios, and create a frame of reference against which we can measure future security metrics. The rest of this chapter describes the design, testing, and findings from this research. 