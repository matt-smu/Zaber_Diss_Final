

In the literature we reviewed there were over 500 distinct security metrics identified. The surveys each provided their own classification systems which were appropriate for the analysis they conducted, but none of these taxonomies generalize well to classify all types of security metrics. We describe properties common to all metrics, identify overlaps in the various taxonomies, identify points of confusion between existing metric hierarchies, and describe a suitable and intuitive system for classifying any current or future security metric. In using the CyBOK as the underlying classification system we are also able to determine the distribution of metrics in each topic and identify areas of limited coverage which would benefit from future research. 

In reproducing the results from the literature, we faced several issues during implementation, particularly with model based security metrics. Often assumptions were made about the intermediate processing of the pipeline that weren't surfaced in the supporting examples of the publication. We note that many of the survey authors describe security metric validation as an area of concern in security metric research. In response to these concerns and to move forward in our own research, we identified a generalized four step processing pipeline that separates the core steps of this process. By following this workflow we have identified and implemented many reusable preprocessing components, allowing us to rapidly add new security metrics from the literature and immediately test the performance of those metrics against a growing number of input models we use as our reference set. 

By enforcing the 4 stage pipeline abstraction, we achieve several benefits. Each phase is modular so that replacing any piece in the pipeline is straight forward. By plugging in the static reference set to the input phase we create a \textit{unit test} of sorts for our metric library, SecMet. With PTaH, the preprocessing and transformation handlers described above, we can articulate how any or all of the security metrics will behave under a variety of conditions for any given input - not just the reference set we describe above. This, in theory at least, should make characterizing the behaviours of security metrics on internal or sensitive systems as simple as adding input adapters to the existing set, which already includes Nessus, OVAL, NVD, CVE, and now SSFNet. By replacing our validation PTaH with whatever workflow execution engine is already in place, Apache Beam or Storm for example, the SecMet catalog becomes a drop in security measurement aid to support SecDevOps which we refer to as Security Metrics as a Service (S-MaaS).

Our takeaways from the experiments described above indicate that, while validating security metrics is not done rigorously in many of the publications, a mechanism for validation and analysis is not out of reach. By streamlining the development and evaluation process with automation, we aim to lower the barrier to entry in the field and allow researchers to spend more time developing and analyzing security metrics, which in turn should encourage more secure systems being deployed. 

