@article{Fleming_Wallace_1986,
	title        = {How not to lie with statistics: the correct way to summarize benchmark results},
	author       = {Fleming, Philip J. and Wallace, John J.},
	year         = 1986,
	month        = mar,
	journal      = {Communications of the ACM},
	volume       = 29,
	number       = 3,
	pages        = {218–221},
	doi          = {10.1145/5666.5673},
	issn         = {00010782}
}
@article{Oppenheimer_Brown_Traupman_Broadwell_Patterson,
	title        = {Practical Issues in Dependability Benchmarking},
	author       = {Oppenheimer, David and Brown, Aaron B and Traupman, Jonathan and Broadwell, Pete and Patterson, David A},
	pages        = 6,
	abstractnote = {Much of the work to date on dependability benchmarks has focused on costly, comprehensive measurements of whole-system dependability. But benchmarks should also be useful for developers and researchers to quickly evaluate incremental improvements to their systems. To address both audiences, we propose dividing the space of dependability benchmarks into two categories: competitive benchmarks that take the holistic approach, and less expensive developer benchmarks aimed at day-to-day development tasks. In this paper we differentiate the goals of these two types of benchmarks, discuss how each type might be appropriately realized, and propose simplifying assumptions for making them cost-effective.}
}
@inproceedings{Paxson_2004,
	title        = {Strategies for sound internet measurement},
	author       = {Paxson, Vern},
	year         = 2004,
	booktitle    = {Proceedings of the 4th ACM SIGCOMM conference on Internet measurement  - IMC ’04},
	publisher    = {ACM Press},
	pages        = 263,
	doi          = {10.1145/1028788.1028824},
	isbn         = {978-1-58113-821-4},
	url          = {http://portal.acm.org/citation.cfm?doid=1028788.1028824},
	note         = {tex.ids: paxsonStrategiesSoundInternet2004a},
	place        = {Taormina, Sicily, Italy},
	abstractnote = {Conducting an Internet measurement study in a sound fashion can be much more difﬁcult than it might ﬁrst appear. We present a number of strategies drawn from experiences for avoiding or overcoming some of the pitfalls. In particular, we discuss dealing with errors and inaccuracies; the importance of associating meta-data with measurements; the technique of calibrating measurements by examining outliers and testing for consistencies; difﬁculties that arise with large-scale measurements; the utility of developing a discipline for reliably reproducing analysis results; and issues with making datasets publicly available. We conclude with thoughts on the sorts of tools and community practices that can assist researchers with conducting sound measurement studies.}
}
@article{Smith_1988,
	title        = {Characterizing computer performance with a single number},
	author       = {Smith, James E.},
	year         = 1988,
	journal      = {Communications of the ACM},
	volume       = 31,
	number       = 10,
	pages        = {1202–1206}
}
@misc{SSI,
	url          = {https://www.nersc.gov/research-and-development/benchmarking-and-workload-characterization/ssi/}
}
@article{SSI,
	title        = {Single Metric for HPC (NERSC)},
	url          = {https://www.nersc.gov/research-and-development/benchmarking-and-workload-characterization/ssi/}
}
@article{Abubakar_Chiroma_Muaz_Ila_2015,
	title        = {A Review of the Advances in Cyber Security Benchmark Datasets for Evaluating Data-Driven Based Intrusion Detection Systems},
	author       = {Abubakar, Adamu I. and Chiroma, Haruna and Muaz, Sanah Abdullahi and Ila, Libabatu Baballe},
	year         = 2015,
	journal      = {Procedia Computer Science},
	volume       = 62,
	pages        = {221–227},
	doi          = {10.1016/j.procs.2015.08.443},
	issn         = 18770509,
	abstractnote = {Cybercrime has led to the loss of billions of dollars, the malfunctioning of computer systems, the destruction of critical information, the compromising of network integrity and confidentiality, etc. In view of these crimes committed on a daily basis, the security of the computer systems has become imperative to minimize and possibly avoid the impact of cybercrimes. In this paper, we review recent advances in the use of cyber security benchmark datasets for the evaluation of machine learning and data mining-based intrusion detection systems. It was found that the state-of-the-art cyber security benchmark datasets KDD and UNM are no longer reliable, because their datasets cannot meet the expectations of current advances in computer technology. As a result, a new ADFA Linux (ADFA-LD) cyber security benchmark dataset for the evaluation of machine learning and data mining-based intrusion detection systems was proposed in 2013 to meet the current significant advances in computer technology. ADFA-LD requires improvement in terms of full descriptions of its attributes. This review can be used by the research community as a basis for abandoning the previous state-of-the-art cyber security benchmark datasets and starting to use the newly introduced benchmark dataset for effective and robust evaluation of machine learning and data mining-based intrusion detection system.}
}
@article{Amin_Schwartz_Hussain_2013,
	title        = {In quest of benchmarking security risks to cyber-physical systems},
	author       = {Amin, S. and Schwartz, G. A. and Hussain, A.},
	year         = 2013,
	month        = jan,
	journal      = {IEEE Network},
	volume       = 27,
	number       = 1,
	pages        = {19–24},
	doi          = {10.1109/MNET.2013.6423187},
	issn         = {0890-8044},
	abstractnote = {We present a generic yet practical framework for assessing security risks to cyberphysical systems (CPSs). Our framework can be used to benchmark security risks when information is less than perfect, and interdependencies of physical and computational components may result in correlated failures. Such environments are prone to externalities, and can cause huge societal losses. We focus on the risks that arise from interdependent reliability failures (faults) and security failures (attacks). We advocate that a sound assessment of these risks requires explicit modeling of the effects of both technology-based defenses and institutions necessary for supporting them. Thus, we consider technology-based security defenses grounded in information security tools and fault-tolerant control in conjunction with institutional structures. Our game-theoretic approach to estimating security risks facilitates more effective defenses, especially against correlated failures.}
}
@inproceedings{Anisetti_Ardagna_Damiani_Gaudenzi_2017,
	title        = {A Security Benchmark for OpenStack},
	author       = {Anisetti, Marco and Ardagna, Claudio A. and Damiani, Ernesto and Gaudenzi, Filippo},
	year         = 2017,
	month        = jun,
	booktitle    = {2017 IEEE 10th International Conference on Cloud Computing (CLOUD)},
	publisher    = {IEEE},
	pages        = {294–301},
	doi          = {10.1109/CLOUD.2017.45},
	isbn         = {978-1-5386-1993-3},
	url          = {http://ieeexplore.ieee.org/document/8030601/},
	place        = {Honolulu, CA, USA},
	abstractnote = {The cloud computing paradigm entails a radical change in IT provisioning, which must be understood and correctly applied especially when security requirements are considered. Security requirements do not cover anymore just the application itself, but involve the whole cloud supply chain from the hosting infrastructure to the ﬁnal applications. This scenario requires, on one side, new security mechanisms protecting the cloud against misbehaviors/malicious attacks and, on the other side, a continuous and adaptive assurance process evaluating the observed cloud security behavior against the expected one. In this paper, we focus on the evaluation of the security assurance of OpenStack, a major open source cloud infrastructure. We ﬁrst deﬁne a security benchmark for OpenStack, inspired by Center for Internet Security (CIS) benchmark for cloud infrastructures. We then present a platform, called Moon Cloud, for cloud security assurance evaluation, showing an application of our benchmark and platform to the in-production OpenStack deployment of the University of Milan.}
}
@inproceedings{Dumitras_Shou_2011,
	title        = {Toward a standard benchmark for computer security research: the worldwide intelligence network environment (WINE)},
	author       = {Dumitras, Tudor and Shou, Darren},
	year         = 2011,
	booktitle    = {Proceedings of the First Workshop on Building Analysis Datasets and Gathering Experience Returns for Security - BADGERS ’11},
	publisher    = {ACM Press},
	pages        = {89–96},
	doi          = {10.1145/1978672.1978683},
	isbn         = {978-1-4503-0768-0},
	url          = {http://portal.acm.org/citation.cfm?doid=1978672.1978683},
	place        = {Salzburg, Austria},
	abstractnote = {Unlike benchmarks that focus on performance or reliability evaluations, a benchmark for computer security must necessarily include sensitive code and data. Because these artifacts could damage systems or reveal personally identiﬁable information about the users affected by cyber attacks, publicly disseminating such a benchmark raises several scientiﬁc, ethical and legal challenges. We propose the Worldwide Intelligence Network Environment (WINE), a security-benchmarking approach based on rigorous experimental methods. WINE includes representative ﬁeld data, collected worldwide from 240,000 sensors, for new empirical studies, and it will enable the validation of research on all the phases in the lifecycle of security threats. We tackle the key challenges for security benchmarking by designing a platform for repeatable experimentation on the WINE data sets and by collecting the metadata required for understanding the results. In this paper, we review the unique characteristics of the WINE data, we discuss why rigorous benchmarking will provide fresh insights on the security arms race and we propose a research agenda for this area.}
}
@inproceedings{Hlyne_Zavarsky_Butakov_2015,
	title        = {SCAP benchmark for Cisco router security configuration compliance},
	author       = {Hlyne, Chit Nyi Nyi and Zavarsky, Pavol and Butakov, Sergey},
	year         = 2015,
	month        = dec,
	booktitle    = {2015 10th International Conference for Internet Technology and Secured Transactions (ICITST)},
	publisher    = {IEEE},
	pages        = {270–276},
	doi          = {10.1109/ICITST.2015.7412104},
	isbn         = {978-1-908320-52-0},
	url          = {http://ieeexplore.ieee.org/document/7412104/},
	place        = {London, United Kingdom},
	abstractnote = {Information security management is timeconsuming and error-prone. Apart from day-to-day operations, organizations need to comply with industrial regulations or government directives. Thus, organizations are looking for security tools to automate security management tasks and daily operations. Security Content Automation Protocol (SCAP) is a suite of specifications that help to automate security management tasks such as vulnerability measurement and policy compliance evaluation. SCAP benchmark provides detailed guidance on setting the security configuration of network devices, operating systems, and applications. Organizations can use SCAP benchmark to perform automated configuration compliance assessment on network devices, operating systems, and applications. This paper discusses SCAP benchmark components and the development of a SCAP benchmark for automating Cisco router security configuration compliance.}
}
@article{Ring_Wunderlich,
	title        = {Flow-based benchmark data sets for intrusion detection},
	author       = {Ring, Markus and Wunderlich, Sarah and Grüdl, Dominik and Landes, Dieter and Hotho, Andreas},
	pages        = 10,
	note         = {tex.ids: ringFlowbasedBenchmarkDataa},
	abstractnote = {Anomaly based intrusion detection systems suffer from a lack of appropriate evaluation data sets. Often, existing data sets may not be published due to privacy concerns or do not reflect actual and current attack scenarios. In order to overcome these problems, we identify characteristics of good data sets and develop an appropriate concept for the generation of labelled flow-based data sets that satisfy these criteria. The concept is implemented based on OpenStack, thus demonstrating the suitability of virtual environments. Virtual environments offer advantages compared to static data sets by easily creating up-to-date data sets with recent trends in user behaviour and new attack scenarios.}
}
@article{Sharafaldin_Gharib_Lashkari_Ghorbani_2018,
	title        = {Towards a Reliable Intrusion Detection Benchmark Dataset},
	author       = {Sharafaldin, Iman and Gharib, Amirhossein and Lashkari, Arash Habibi and Ghorbani, Ali A.},
	year         = 2018,
	month        = jan,
	journal      = {Software Networking},
	volume       = 2018,
	number       = 1,
	pages        = {177–200},
	doi          = {10.13052/jsn2445-9739.2017.009},
	issn         = {2445-9739},
	abstractnote = {Towards a Reliable Intrusion Detection Benchmark Dataset}
}
@inproceedings{cipher_benchmark,
	title        = {Survey and Benchmark of Lightweight Block Ciphers for Wireless Sensor Networks},
	year         = 2013,
	booktitle    = {Proceedings of the 10th International Conference on Security and Cryptography},
	publisher    = {SCITEPRESS - Science and and Technology Publications},
	pages        = {543–548},
	doi          = {10.5220/0004530905430548},
	isbn         = {978-989-8565-73-0},
	url          = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0004530905430548},
	place        = {Reykjavík, Iceland}
}
@report{zaber_pkb,
	title        = {Measuring Cloud Network Performance with PerfKit Benchmarker},
	author       = {Derek Phanekham and Matthew Zaber and Suku Nair}
}
@article{Wald_1980,
	title        = {A method of estimating plane vulnerability based on damage of survivors, CRC 432, July 1980},
	author       = {Wald, A.},
	year         = 1980,
	journal      = {Center for Naval Analyses}
}
