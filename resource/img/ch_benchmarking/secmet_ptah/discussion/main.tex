% Here you must respond to what the results mean. Probably it is the easiest section to write, but the hardest section to get right. This is because it is the most important section of your article. Here you get the chance to sell your data. Take into account that a huge numbers of manuscripts are rejected because the Discussion is weak.

% You need to make the Discussion corresponding to the Results, but do not reiterate the results. Here you need to compare the published results by your colleagues with yours (using some of the references included in the Introduction). Never ignore work in disagreement with yours, in turn, you must confront it and convince the reader that you are correct or better.

% Take into account the following tips:

% \begin{enumerate}
% \item  Avoid statements that go beyond what the results can support.
% \item Avoid unspecific expressions such as "higher temperature", "at a lower rate", "highly significant". Quantitative descriptions are always preferred.
% \item Avoid sudden introduction of new terms or ideas; you must present everything in the introduction, to be confronted with your results here.
% \item  Speculations on possible interpretations are allowed, but these should be rooted in fact, rather than imagination. To achieve good interpretations think about:  How do these results relate to the original question or objectives outlined in the Introduction section?
%     Do the data support your hypothesis?
%     Are your results consistent with what other investigators have reported?
%     Discuss weaknesses and discrepancies. If your results were unexpected, try to explain why
%     Is there another way to interpret your results?
%     What further research would be necessary to answer the questions raised by your results?
%     Explain what is new without exaggerating
% \item  Revision of Results and Discussion is not just paper work. You may do further experiments, derivations, or simulations. Sometimes you cannot clarify your idea in words because some critical items have not been studied substantially
% \end{enumerate}


% \subsection{Validation Methodology}\label{sec:results:validation}
% \input{content/results/validation.tex}


% \subsection{Security Metrics as a Service}\label{sec:results:smaas}
% \input{content/results/smaas.tex}


In the security metrics surveys we reviewed in Section \ref{sec:litreview} there were over 500 distinct security metrics identified. The surveys each provided their own classification systems which were appropriate for the analysis they conducted, but none of these taxonomies generalize well to classify all types of security metrics. We describe properties common to all metrics, identify overlaps in the various taxonomies, identify points of confusion between existing metric hierarchies, and describe a suitable and intuitive system for classifying any current or future security metric. In using the CyBOK as the underlying classification system we are also able to determine the distribution of metrics in each topic and identify areas of limited coverage which would benefit from future research. 

In reproducing the results from the literature, we faced several issues during implementation, particularly with model based security metrics. Often assumptions were made about the intermediate processing of the pipeline that weren't surfaced in the supporting examples of the publication. We note that many of the survey authors describe security metric validation as an area of concern in security metric research. In response to these concerns and to move forward in our own research, we identified a generalized four step processing pipeline that separates the core steps of this process. By following this workflow we have identified and implemented many reusable preprocessing components, allowing us to rapidly add new security metrics from the literature and immediately test the performance of those metrics against a growing number of input models we use as our reference set. 

By enforcing the 4 stage pipeline abstraction, we achieve several benefits. Each phase is modular so that replacing any piece in the pipeline is straight forward. By plugging in the static reference set to the input phase we create a \textit{unit test} of sorts for our metric library, SecMet. With PTaH, the preprocessing and transformation handlers described above, we can articulate how any or all of the security metrics will behave under a variety of conditions for any given input - not just the reference set we describe above. This, in theory at least, should make characterizing the behaviours of security metrics on internal or sensitive systems as simple as adding input adapters to the existing set, which already includes Nessus, OVAL, NVD, CVE, and now SSFNet. By replacing our validation PTaH with whatever workflow execution engine is already in place, Apache Beam or Storm for example, the SecMet catalog becomes a drop in security measurement aid to support SecDevOps which we refer to as Security Metrics as a Service (S-MaaS).

Our takeaways from the experiments described above indicate that, while validating security metrics is not done rigorously in many of the publications, a mechanism for validation and analysis is not out of reach. By streamlining the development and evaluation process with automation, we aim to lower the barrier to entry in the field and allow researchers to spend more time developing and analyzing security metrics, which in turn should encourage more secure systems being deployed. 



